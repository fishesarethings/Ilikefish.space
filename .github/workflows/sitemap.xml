name: Generate Sitemap

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build-sitemap:
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Generate sitemap.xml and ensure robots.txt
        env:
          BASE_URL: "https://ilikefish.space"
        run: |
          set -euo pipefail

          mkdir -p .github/tmp
          cat > .github/tmp/generate-sitemap.js <<'NODE'
          const fs = require('fs');
          const path = require('path');

          const BASE = process.env.BASE_URL || 'https://ilikefish.space';
          const staticPages = ['/', '/games.html', '/gamepack.html', '/terms.html'];

          const xmlEscape = s => String(s)
            .replace(/&/g,'&amp;')
            .replace(/</g,'&lt;')
            .replace(/>/g,'&gt;')
            .replace(/"/g,'&quot;')
            .replace(/'/g,'&apos;');

          const urls = [];

          for (const p of staticPages) {
            urls.push({ loc: BASE + p, changefreq: 'weekly', priority: '0.8' });
          }

          const indexPath = path.join(process.cwd(), 'games', 'index.json');
          let index = null;
          if (fs.existsSync(indexPath)) {
            try { index = JSON.parse(fs.readFileSync(indexPath, 'utf8')); }
            catch (e) { console.warn('games/index.json parse failed:', e.message); index = null; }
          }

          if (index && Array.isArray(index.folders)) {
            for (const slug of index.folders) {
              try {
                const cfgPath = path.join('games', slug, 'config.json');
                if (fs.existsSync(cfgPath)) {
                  const cfg = JSON.parse(fs.readFileSync(cfgPath, 'utf8'));
                  let entry = cfg.entry && String(cfg.entry).trim() ? String(cfg.entry).trim() : '';
                  entry = entry.replace(/^\/+/, '');
                  if (entry) {
                    urls.push({
                      loc: BASE + '/games/' + slug + '/' + entry,
                      changefreq: 'monthly',
                      priority: '0.7'
                    });
                  } else {
                    urls.push({
                      loc: BASE + '/games/' + slug + '/',
                      changefreq: 'monthly',
                      priority: '0.6'
                    });
                  }
                } else {
                  urls.push({
                    loc: BASE + '/games/' + slug + '/',
                    changefreq: 'monthly',
                    priority: '0.5'
                  });
                }
              } catch (err) {
                console.warn('Warning: failed to handle game', slug, err && err.message);
              }
            }
          }

          const now = new Date().toISOString();
          const header = '<?xml version="1.0" encoding="UTF-8"?>\n' +
            '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n';
          const footer = '</urlset>\n';
          const body = urls.map(u => {
            return '  <url>\n' +
                   '    <loc>' + xmlEscape(u.loc) + '</loc>\n' +
                   '    <lastmod>' + now + '</lastmod>\n' +
                   '    <changefreq>' + u.changefreq + '</changefreq>\n' +
                   '    <priority>' + u.priority + '</priority>\n' +
                   '  </url>';
          }).join('\n');
          const xml = header + body + '\n' + footer;

          fs.writeFileSync('sitemap.xml', xml, 'utf8');
          console.log('Wrote sitemap.xml with', urls.length, 'entries');

          const robotsPath = path.join(process.cwd(), 'robots.txt');
          const sitemapLine = 'Sitemap: ' + BASE + '/sitemap.xml\n';
          let robotsContent = '';
          if (fs.existsSync(robotsPath)) {
            robotsContent = fs.readFileSync(robotsPath, 'utf8');
            if (!/Sitemap:/i.test(robotsContent)) {
              robotsContent = robotsContent.trim() + '\n\n' + sitemapLine;
              fs.writeFileSync(robotsPath, robotsContent, 'utf8');
              console.log('Updated robots.txt with Sitemap line');
            } else {
              console.log('robots.txt already contains a Sitemap entry');
            }
          } else {
            robotsContent = 'User-agent: *\nAllow: /\n\n' + sitemapLine;
            fs.writeFileSync(robotsPath, robotsContent, 'utf8');
            console.log('Created robots.txt');
          }
          NODE

          node .github/tmp/generate-sitemap.js

      - name: Check for changes
        id: check_changes
        run: |
          git add sitemap.xml robots.txt || true
          if git diff --cached --quiet; then
            echo "no_changes=true" >> $GITHUB_OUTPUT
          else
            echo "no_changes=false" >> $GITHUB_OUTPUT
          fi

      - name: Commit & push if changed
        if: steps.check_changes.outputs.no_changes == 'false'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git commit -m "chore(sitemap): update sitemap.xml and robots.txt [skip ci]" || echo "nothing to commit"
          git push origin HEAD:main

      - name: Done
        run: echo "Sitemap workflow finished."
